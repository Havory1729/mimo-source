
------------------------------Adam training loop-------------------

Epoch: 0, lr: 0.001, Loss: 1.384, Acc: 0.360
Epoch: 50, lr: 0.001, Loss: 1.372, Acc: 0.430
Epoch: 100, lr: 0.001, Loss: 1.357, Acc: 0.430
Epoch: 150, lr: 0.001, Loss: 1.335, Acc: 0.430
Epoch: 200, lr: 0.001, Loss: 1.307, Acc: 0.440
Epoch: 250, lr: 0.001, Loss: 1.273, Acc: 0.490
Epoch: 300, lr: 0.001, Loss: 1.233, Acc: 0.540
Epoch: 350, lr: 0.001, Loss: 1.188, Acc: 0.580
Epoch: 400, lr: 0.001, Loss: 1.137, Acc: 0.640
Epoch: 450, lr: 0.001, Loss: 1.083, Acc: 0.690
Epoch: 500, lr: 0.001, Loss: 1.024, Acc: 0.770
Epoch: 550, lr: 0.001, Loss: 0.963, Acc: 0.780
Epoch: 600, lr: 0.001, Loss: 0.901, Acc: 0.840
Epoch: 650, lr: 0.001, Loss: 0.838, Acc: 0.840
Epoch: 700, lr: 0.001, Loss: 0.774, Acc: 0.870
Epoch: 750, lr: 0.001, Loss: 0.712, Acc: 0.920
Epoch: 800, lr: 0.001, Loss: 0.650, Acc: 0.960
Epoch: 850, lr: 0.001, Loss: 0.592, Acc: 0.960
Epoch: 900, lr: 0.001, Loss: 0.538, Acc: 0.980
Epoch: 950, lr: 0.001, Loss: 0.488, Acc: 0.980
Epoch: 1000, lr: 0.001, Loss: 0.443, Acc: 0.990
Epoch: 1050, lr: 0.001, Loss: 0.402, Acc: 0.990
Epoch: 1100, lr: 0.001, Loss: 0.366, Acc: 0.990
Epoch: 1150, lr: 0.001, Loss: 0.333, Acc: 0.990
Epoch: 1200, lr: 0.001, Loss: 0.304, Acc: 0.990
Epoch: 1250, lr: 0.001, Loss: 0.279, Acc: 0.990
Epoch: 1300, lr: 0.001, Loss: 0.255, Acc: 0.990
Epoch: 1350, lr: 0.001, Loss: 0.235, Acc: 1.000
Epoch: 1400, lr: 0.001, Loss: 0.216, Acc: 1.000
Epoch: 1450, lr: 0.001, Loss: 0.199, Acc: 1.000
Epoch: 1500, lr: 0.001, Loss: 0.183, Acc: 1.000
Epoch: 1550, lr: 0.001, Loss: 0.170, Acc: 1.000
Epoch: 1600, lr: 0.001, Loss: 0.157, Acc: 1.000
Epoch: 1650, lr: 0.001, Loss: 0.146, Acc: 1.000
Epoch: 1700, lr: 0.001, Loss: 0.136, Acc: 1.000
Epoch: 1750, lr: 0.001, Loss: 0.127, Acc: 1.000
Epoch: 1800, lr: 0.001, Loss: 0.119, Acc: 1.000
Epoch: 1850, lr: 0.001, Loss: 0.112, Acc: 1.000
Epoch: 1900, lr: 0.001, Loss: 0.105, Acc: 1.000
Epoch: 1950, lr: 0.001, Loss: 0.099, Acc: 1.000
Epoch: 2000, lr: 0.001, Loss: 0.094, Acc: 1.000
Epoch: 2050, lr: 0.001, Loss: 0.089, Acc: 1.000
Epoch: 2100, lr: 0.001, Loss: 0.084, Acc: 1.000
Epoch: 2150, lr: 0.001, Loss: 0.080, Acc: 1.000
Epoch: 2200, lr: 0.001, Loss: 0.076, Acc: 1.000
Epoch: 2250, lr: 0.001, Loss: 0.072, Acc: 1.000
Epoch: 2300, lr: 0.001, Loss: 0.069, Acc: 1.000
Epoch: 2350, lr: 0.001, Loss: 0.066, Acc: 1.000
Epoch: 2400, lr: 0.001, Loss: 0.063, Acc: 1.000
Epoch: 2450, lr: 0.001, Loss: 0.060, Acc: 1.000
Epoch: 2500, lr: 0.001, Loss: 0.057, Acc: 1.000
Epoch: 2550, lr: 0.001, Loss: 0.055, Acc: 1.000
Epoch: 2600, lr: 0.001, Loss: 0.053, Acc: 1.000
Epoch: 2650, lr: 0.001, Loss: 0.050, Acc: 1.000
Epoch: 2700, lr: 0.001, Loss: 0.048, Acc: 1.000
Epoch: 2750, lr: 0.001, Loss: 0.046, Acc: 1.000
Epoch: 2800, lr: 0.001, Loss: 0.045, Acc: 1.000
Epoch: 2850, lr: 0.001, Loss: 0.043, Acc: 1.000
Epoch: 2900, lr: 0.001, Loss: 0.041, Acc: 1.000
Epoch: 2950, lr: 0.001, Loss: 0.040, Acc: 1.000
Epoch: 3000, lr: 0.001, Loss: 0.038, Acc: 1.000

-----------------------Adam testing result-----------------------

loss= 0.03817449087154783, acc= 1.0

---------------RMSProp training loop-----------------

Epoch: 0, lr: 0.001, Loss: 1.237, Acc: 0.660
Epoch: 50, lr: 0.001, Loss: 0.044, Acc: 0.980
Epoch: 100, lr: 0.001, Loss: 0.001, Acc: 1.000
Epoch: 150, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 200, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 250, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 300, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 350, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 400, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 450, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 500, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 550, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 600, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 650, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 700, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1000, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1050, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1100, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1150, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1200, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1250, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1300, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1350, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1400, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1450, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1500, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1550, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1600, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1650, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1700, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2000, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2050, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2100, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2150, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2200, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2250, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2300, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2350, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2400, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2450, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2500, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2550, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2600, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2650, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2700, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 3000, lr: 0.001, Loss: 0.000, Acc: 1.000

-----------------------RMSProp testing result-----------------------

loss= 1.2445335173071612e-07, acc= 1.0

---------------Adagrad training loop-----------------

Epoch: 0, lr: 0.001, Loss: 2.719, Acc: 0.790
Epoch: 50, lr: 0.001, Loss: 2.391, Acc: 0.840
Epoch: 100, lr: 0.001, Loss: 2.119, Acc: 0.850
Epoch: 150, lr: 0.001, Loss: 1.903, Acc: 0.860
Epoch: 200, lr: 0.001, Loss: 1.755, Acc: 0.870
Epoch: 250, lr: 0.001, Loss: 1.601, Acc: 0.870
Epoch: 300, lr: 0.001, Loss: 1.449, Acc: 0.880
Epoch: 350, lr: 0.001, Loss: 1.340, Acc: 0.890
Epoch: 400, lr: 0.001, Loss: 1.201, Acc: 0.890
Epoch: 450, lr: 0.001, Loss: 1.109, Acc: 0.910
Epoch: 500, lr: 0.001, Loss: 0.966, Acc: 0.910
Epoch: 550, lr: 0.001, Loss: 0.826, Acc: 0.930
Epoch: 600, lr: 0.001, Loss: 0.804, Acc: 0.930
Epoch: 650, lr: 0.001, Loss: 0.782, Acc: 0.940
Epoch: 700, lr: 0.001, Loss: 0.743, Acc: 0.940
Epoch: 750, lr: 0.001, Loss: 0.657, Acc: 0.940
Epoch: 800, lr: 0.001, Loss: 0.494, Acc: 0.950
Epoch: 850, lr: 0.001, Loss: 0.385, Acc: 0.960
Epoch: 900, lr: 0.001, Loss: 0.326, Acc: 0.970
Epoch: 950, lr: 0.001, Loss: 0.295, Acc: 0.980
Epoch: 1000, lr: 0.001, Loss: 0.269, Acc: 0.980
Epoch: 1050, lr: 0.001, Loss: 0.204, Acc: 0.980
Epoch: 1100, lr: 0.001, Loss: 0.096, Acc: 0.990
Epoch: 1150, lr: 0.001, Loss: 0.019, Acc: 0.990
Epoch: 1200, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1250, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1300, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1350, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1400, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1450, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1500, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1550, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1600, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1650, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1700, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 1950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2000, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2050, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2100, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2150, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2200, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2250, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2300, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2350, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2400, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2450, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2500, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2550, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2600, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2650, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2700, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 3000, lr: 0.001, Loss: 0.000, Acc: 1.000

-----------------------Adagrad testing result-----------------------

loss= 6.711846418642694e-07, acc= 1.0

---------------SGD training loop-----------------

Epoch: 0, lr: 0.001, Loss: 2.844, Acc: 0.790
Epoch: 50, lr: 0.001, Loss: 2.627, Acc: 0.790
Epoch: 100, lr: 0.001, Loss: 2.427, Acc: 0.800
Epoch: 150, lr: 0.001, Loss: 2.230, Acc: 0.800
Epoch: 200, lr: 0.001, Loss: 2.083, Acc: 0.830
Epoch: 250, lr: 0.001, Loss: 1.957, Acc: 0.850
Epoch: 300, lr: 0.001, Loss: 1.794, Acc: 0.850
Epoch: 350, lr: 0.001, Loss: 1.667, Acc: 0.880
Epoch: 400, lr: 0.001, Loss: 1.595, Acc: 0.880
Epoch: 450, lr: 0.001, Loss: 1.499, Acc: 0.880
Epoch: 500, lr: 0.001, Loss: 1.405, Acc: 0.890
Epoch: 550, lr: 0.001, Loss: 1.321, Acc: 0.890
Epoch: 600, lr: 0.001, Loss: 1.211, Acc: 0.890
Epoch: 650, lr: 0.001, Loss: 1.079, Acc: 0.900
Epoch: 700, lr: 0.001, Loss: 0.942, Acc: 0.910
Epoch: 750, lr: 0.001, Loss: 0.845, Acc: 0.920
Epoch: 800, lr: 0.001, Loss: 0.775, Acc: 0.930
Epoch: 850, lr: 0.001, Loss: 0.738, Acc: 0.930
Epoch: 900, lr: 0.001, Loss: 0.698, Acc: 0.930
Epoch: 950, lr: 0.001, Loss: 0.655, Acc: 0.930
Epoch: 1000, lr: 0.001, Loss: 0.622, Acc: 0.930
Epoch: 1050, lr: 0.001, Loss: 0.600, Acc: 0.940
Epoch: 1100, lr: 0.001, Loss: 0.584, Acc: 0.940
Epoch: 1150, lr: 0.001, Loss: 0.568, Acc: 0.950
Epoch: 1200, lr: 0.001, Loss: 0.555, Acc: 0.950
Epoch: 1250, lr: 0.001, Loss: 0.541, Acc: 0.950
Epoch: 1300, lr: 0.001, Loss: 0.522, Acc: 0.950
Epoch: 1350, lr: 0.001, Loss: 0.501, Acc: 0.960
Epoch: 1400, lr: 0.001, Loss: 0.484, Acc: 0.960
Epoch: 1450, lr: 0.001, Loss: 0.475, Acc: 0.960
Epoch: 1500, lr: 0.001, Loss: 0.468, Acc: 0.970
Epoch: 1550, lr: 0.001, Loss: 0.461, Acc: 0.970
Epoch: 1600, lr: 0.001, Loss: 0.458, Acc: 0.970
Epoch: 1650, lr: 0.001, Loss: 0.457, Acc: 0.970
Epoch: 1700, lr: 0.001, Loss: 0.456, Acc: 0.970
Epoch: 1750, lr: 0.001, Loss: 0.455, Acc: 0.970
Epoch: 1800, lr: 0.001, Loss: 0.454, Acc: 0.970
Epoch: 1850, lr: 0.001, Loss: 0.451, Acc: 0.970
Epoch: 1900, lr: 0.001, Loss: 0.447, Acc: 0.970
Epoch: 1950, lr: 0.001, Loss: 0.443, Acc: 0.970
Epoch: 2000, lr: 0.001, Loss: 0.436, Acc: 0.970
Epoch: 2050, lr: 0.001, Loss: 0.426, Acc: 0.970
Epoch: 2100, lr: 0.001, Loss: 0.411, Acc: 0.970
Epoch: 2150, lr: 0.001, Loss: 0.388, Acc: 0.970
Epoch: 2200, lr: 0.001, Loss: 0.355, Acc: 0.970
Epoch: 2250, lr: 0.001, Loss: 0.311, Acc: 0.970
Epoch: 2300, lr: 0.001, Loss: 0.258, Acc: 0.970
Epoch: 2350, lr: 0.001, Loss: 0.187, Acc: 0.970
Epoch: 2400, lr: 0.001, Loss: 0.130, Acc: 0.990
Epoch: 2450, lr: 0.001, Loss: 0.082, Acc: 0.990
Epoch: 2500, lr: 0.001, Loss: 0.031, Acc: 0.990
Epoch: 2550, lr: 0.001, Loss: 0.006, Acc: 1.000
Epoch: 2600, lr: 0.001, Loss: 0.002, Acc: 1.000
Epoch: 2650, lr: 0.001, Loss: 0.001, Acc: 1.000
Epoch: 2700, lr: 0.001, Loss: 0.001, Acc: 1.000
Epoch: 2750, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2800, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2850, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2900, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 2950, lr: 0.001, Loss: 0.000, Acc: 1.000
Epoch: 3000, lr: 0.001, Loss: 0.000, Acc: 1.000

-----------------------SGD testing result-----------------------

loss= 1.4677230946917713e-05, acc= 1.0
